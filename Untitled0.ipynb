{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVJ/94xNvUYngBxZEGt5/Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# univariate multi-step encoder-decoder lstm example\n","from numpy import array\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","\n","#from google.colab import files\n","#iles.upload()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import sys\n","import random\n","from sklearn.metrics import recall_score, roc_curve, auc, accuracy_score, confusion_matrix, precision_score, f1_score\n","\n","# RTP Mining \n","# C1: positive class , C0: negative class \n","def store_patterns(i, trainC1, trainC0, opts):\n","\n","    # -------------------------------\n","    # RTP mining for each class\n","    C1_patterns = RTPmining.pattern_mining(trainC1, opts.max_g, opts.sup_pos*len(trainC1), opts)\n","    print(\"Total # patterns from positive:\", len(C1_patterns))\n","    C0_patterns = RTPmining.pattern_mining(trainC0, opts.max_g, opts.sup_neg*len(trainC0), opts)\n","    print(\"Total # patterns from negative:\", len(C0_patterns))\n","\n","    ############## Writing patterns to the files #################\n","    C1_pos_file = open(opts.patterns_path + 'C1_pos_'+opts.alignment+'_fold'+str(i)+'_'+str(opts.early_prediction)+'.txt','w')\n","    C0_neg_file = open(opts.patterns_path + 'C0_neg_'+opts.alignment+'_fold'+str(i)+'_'+str(opts.early_prediction)+'.txt','w')\n","    for p in C1_patterns:\n","        C1_pos_file.write(p.describe())\n","    for p in C0_patterns:\n","        C0_neg_file.write(p.describe())\n","\n","    pos_fname = opts.patterns_path + 'C1_pos_'+opts.alignment+'_fold'+str(i)+'_'+str(opts.early_prediction)+'.pckl'\n","    neg_fname = opts.patterns_path + 'C0_neg_'+opts.alignment+'_fold'+str(i)+'_'+str(opts.early_prediction)+'.pckl'\n","\n","    f = open(pos_fname, 'wb')\n","    pickle.dump(C1_patterns, f)\n","    f.close()\n","    f = open(neg_fname, 'wb')\n","    pickle.dump(C0_patterns, f)\n","    f.close()\n","\n","    return C1_patterns, C0_patterns\n","\n","def load_patterns(i, opts):\n","    pos_fname = opts.patterns_path + 'C1_pos_'+opts.alignment+'_fold'+str(i)+'_'+str(opts.early_prediction)+'.pckl'\n","    neg_fname = opts.patterns_path + 'C0_neg_'+opts.alignment+'_fold'+str(i)+'_'+str(opts.early_prediction)+'.pckl'\n","    f = open(pos_fname, 'rb')\n","    C1_patterns = pickle.load(f)\n","    f.close()\n","    f = open(neg_fname, 'rb')\n","    C0_patterns = pickle.load(f)\n","    f.close()\n","    return C1_patterns, C0_patterns\n","\n","def random_subset(iterator, K):\n","    result = []\n","    N = 0\n","    for item in iterator:\n","        N += 1\n","        if len( result ) < K:\n","            result.append( item )\n","        else:\n","            s = int(random.random() * N)\n","            if s < K:\n","                result[ s ] = item\n","    return result\n","\n","\n","def make_MSS(pos_events, neg_events, opts):\n","    # -------------------------------------------------------    \n","    # 0. Data Preprocessiong\n","    # -------------------------------------------------------    \n","    # 0-1. Data alignment \n","    # Right aligned: a backward time window from the shock onset time\n","    if opts.early_prediction > 0 and opts.alignment == 'right':\n","        pos_cut = pos_events[pos_events.EventTime - pos_events[opts.timestamp_variable] >= opts.early_prediction * 60].copy(deep=True)\n","        neg_cut = neg_events[neg_events.LastMinute - neg_events[opts.timestamp_variable] >= opts.early_prediction * 60].copy(deep=True)\n","        if opts.observation_window:\n","            pos_cut = pos_cut[pos_cut.EventTime - pos_cut[opts.timestamp_variable] <= 60 * (opts.observation_window + opts.early_prediction)]\n","            neg_cut = neg_cut[neg_cut.LastMinute - neg_cut[opts.timestamp_variable] <= 60 * (opts.observation_window + opts.early_prediction)]\n","            \n","    # Left aligned: a time window starting from the begning of the trajectory\n","    elif opts.observation_window and opts.alignment == 'left':\n","        pos_cut = pos_events[pos_events[opts.timestamp_variable] <= opts.early_prediction * 60]\n","        neg_cut = neg_events[neg_events[opts.timestamp_variable] <= opts.early_prediction * 60]\n","    \n","    # When using 'trunc' mode, we use truncated data for both training and test     \n","    if opts.settings == 'trunc':\n","        pos_events = pos_cut.copy(deep=True)\n","        neg_events = neg_cut.copy(deep=True)\n","\n","    # 0-2. Sampling for balanced data (because we may lose some samples by data alignment)      \n","    # Sampling from negative visits, if # of negative trajectories > # of positive trajectories        \n","    if len(neg_events[opts.unique_id_variable].unique()) > len(pos_events[opts.unique_id_variable].unique()):\n","        neg_id = random_subset(neg_events[opts.unique_id_variable].unique(), len(pos_events[opts.unique_id_variable].unique()))\n","        neg_events = neg_events[neg_events[opts.unique_id_variable].isin(neg_id)]\n","        \n","    # Sampling from positive visits, if # of positives > # of negatives        \n","    if len(pos_events[opts.unique_id_variable].unique()) > len(neg_events[opts.unique_id_variable].unique()):\n","        pos_id = random_subset(pos_events[opts.unique_id_variable].unique(), len(neg_events[opts.unique_id_variable].unique()))\n","        pos_events = pos_events[pos_events[opts.unique_id_variable].isin(pos_id)]\n","\n","    # -------------------------------------------------------    \n","    #  1. Temporal Abstraction\n","    # -------------------------------------------------------    \n","    #  1-1. Discretize numerical values and abstract temporal states over time (please see the lecture 6)\n","    for f in opts.numerical_feat:\n","        pos_events.loc[:,f], neg_events.loc[:,f] = TemporalAbstraction.abstraction_alphabet(pos_events[f], neg_events[f])\n","    \n","    # When using 'entire' mode, we need truncated data for test.     \n","    if opts.settings == 'entire':    \n","        for f in opts.numerical_feat:\n","            pos_cut.loc[:,f], neg_cut.loc[:,f] = TemporalAbstraction.abstraction_alphabet(pos_cut[f], neg_cut[f])\n","\n","    \n","    # -------------------------------------------------------    \n","    #  1-2. Make Multivariate State Sequence (MSS), consisting of State Intervals\n","    #  State Interval = (feature, value, start, end)\n","\n","    #  MSS for positive cases\n","    MSS_positive = []\n","    grouped = pos_events.groupby(opts.unique_id_variable)\n","    for name, group in grouped:\n","        group = group.sort_values([opts.timestamp_variable])\n","        MSS_positive.append(TemporalAbstraction.MultivariateStateSequence(group, opts))\n","    #print(len(MSS_positive))\n","\n","    #  MSS for negative cases\n","    MSS_negative = []\n","    grouped = neg_events.groupby(opts.unique_id_variable)\n","    for name, group in grouped:\n","        group = group.sort_values([opts.timestamp_variable])\n","        MSS_negative.append(TemporalAbstraction.MultivariateStateSequence(group, opts))\n","    #print(len(MSS_negative))\n","    \n","    # Save the generated MSSs for each class (to skip this long process when comparing different classifiers)\n","    f = open('RTP_log/MSS_pos_'+opts.alignment+'_'+str(opts.early_prediction)+'.pckl', 'wb')\n","    pickle.dump(MSS_positive, f)\n","    f.close()\n","    f = open('RTP_log/MSS_neg_'+opts.alignment+'_'+str(opts.early_prediction)+'.pckl', 'wb')\n","    pickle.dump(MSS_negative, f)\n","    f.close()\n","\n","    return MSS_positive, MSS_negative\n","\n","\n","def load_MSS(opts):\n","    f = open('RTP_log/MSS_pos_'+opts.alignment+'_'+str(opts.early_prediction)+'.pckl', 'rb')\n","    MSS_positive = pickle.load(f)\n","    f.close()\n","    f = open('RTP_log/MSS_neg_'+opts.alignment+'_'+str(opts.early_prediction)+'.pckl', 'rb')\n","    MSS_negative = pickle.load(f)\n","    f.close()\n","\n","    return MSS_positive, MSS_negative\n","\n","\n","# k-fold cross validataion (the k-fold is definece in Config.py)\n","\n","def pred_fold(MSS_positive, MSS_negative, opts):\n","    \n","    \n","    if opts.alignment == 'right':\n","        if len(MSS_negative) > len(MSS_positive):\n","            MSS_negative = random_subset(MSS_negative, len(MSS_positive))\n","\n","    print (\"size of negative data:\", len(MSS_negative))\n","    print (\"size of positive data:\", len(MSS_positive))\n","\n","    C0_subset_size = len(MSS_negative)//opts.num_folds # C0 = negative class\n","    C1_subset_size = len(MSS_positive)//opts.num_folds # C1 = positive class\n","\n","    test_pred, test_pred_prob, train_pred, test_labels = [], [], [], []\n","    all_test_labels = []\n","    \n","    for i in range(opts.num_folds):\n","        print( \"***************** FOLD \", i+1, \"*****************\")      \n","        trainC0 = MSS_negative[:i*C0_subset_size] + MSS_negative[(i+1)*C0_subset_size:]\n","        trainC1 = MSS_positive[:i*C1_subset_size] + MSS_positive[(i+1)*C1_subset_size:]\n","        \n","        if opts.settings == 'trunc':\n","            testC0 = MSS_negative[i*C0_subset_size:][:C0_subset_size]\n","            testC1 = MSS_positive[i*C1_subset_size:][:C1_subset_size]\n","        \n","        print (\"Size of positive and negative training:\", len(trainC1), len(trainC0))\n","        print( \"Size of positive and negative test:\", len(testC1), len(testC0))       \n","        \n","        # ------------------------------------------------\n","        # 2. RTP (Recent Temporal Pattern) mining\n","        \n","        # ---- either store patterns or load the dumped ones\n","        C1_patterns, C0_patterns = store_patterns(i, trainC1, trainC0, opts)\n","        # C1_patterns, C0_patterns = load_patterns(i, opts)\n","        # ------------------------------------------------\n","        \n","        # Get all the unique patterns\n","        all_patterns = list(C1_patterns)\n","        for j in range(0,len(C0_patterns)):\n","            if not any((x == C0_patterns[j]) for x in all_patterns):\n","                all_patterns.append(C0_patterns[j])\n","        print (\"number of all patterns:\", len(all_patterns))\n","\n","        # Make the training and test set\n","        train_data = list(trainC1)\n","        train_data.extend(trainC0)\n","        train_labels = list(np.ones(len(trainC1)))\n","        train_labels.extend(np.zeros(len(trainC0)))\n","\n","        test_data = list(testC1)\n","        test_data.extend(testC0)\n","        test_labels = list(np.ones(len(testC1)))\n","        test_labels.extend(np.zeros(len(testC0)))\n","        \n","        # 3. Data Transformation\n","        train_binary_matrix = pd.DataFrame(classifier.create_binary_matrix(train_data, all_patterns, opts.max_g))\n","        test_binary_matrix = pd.DataFrame(classifier.create_binary_matrix(test_data, all_patterns, opts.max_g))\n","\n","        # 4. Classification\n","        trp, tsp, tsp_prob = classifier.learn_classifier(opts, train_binary_matrix, train_labels, test_binary_matrix, test_labels)\n","        #trp, tsp, tsp_prob = classifier.learn_logistic_regression(train_binary_matrix, train_labels, test_binary_matrix, test_labels)\n","        print (len(tsp))\n","        all_test_labels.extend(test_labels)\n","        test_pred.extend(tsp)\n","        train_pred.extend(trp)\n","        for each in tsp_prob:\n","            test_pred_prob.append(each[1])\n","        print(len(tsp_prob))\n","        \n","    print(confusion_matrix(all_test_labels, test_pred))\n","    \n","    accuracy = accuracy_score(all_test_labels, test_pred)\n","    precision = precision_score(all_test_labels, test_pred)\n","    recall = recall_score(all_test_labels, test_pred)\n","    f_measure = f1_score(all_test_labels, test_pred)\n","    \n","    fpr, tpr, _ = roc_curve(all_test_labels, test_pred_prob)\n","    roc_auc = auc(fpr, tpr)\n","    return accuracy, precision, recall, f_measure, roc_auc\n","\n","\n","\n","# According to the recommendation of medical experts: \n","# Forward filling for 8 hours for vital signs and 24 hours for lab results\n","def expertImputation(opts):\n","    ndf_ni = pd.read_csv('MyDrive/mimic/mimic_nonshock_noImpute.csv',header=0)\n","    sdf_ni = pd.read_csv('MyDrive/mimic/mimic_shock_noImpute.csv',header=0)\n","    # ---------------------------------------------------------------------------\n","    # Assume that we are given the mean values from the training set as follows:    \n","    df = pd.concat([ndf_ni, sdf_ni], axis=0, sort=False)\n","    meanTrain = df.groupby(opts.unique_id_variable)[opts.all_feat].mean().mean().values\n","    # ---------------------------------------------------------------------------    \n","    \n","    for f in opts.numerical_feat: \n","        if f in opts.vitals or f in opts.oxygenCtrl:\n","            ffill_window = 8 * 60\n","        elif f in opts.labs:\n","            ffill_window = 24 * 60\n","            \n","        ndf = carryFwd_minutes(opts, ndf_ni, f, ffill_window)\n","        sdf = carryFwd_minutes(opts, sdf_ni, f, ffill_window)\n","\n","    # Mean imputation\n","    sdf = meanImpute(sdf, opts.all_feat, meanTrain)\n","    ndf = meanImpute(ndf, opts.all_feat, meanTrain)\n","        \n","    ndf.to_csv('MyDrive/mimic/mimic_nonshock.csv', index=False)\n","    sdf.to_csv('MyDrive/mimic/mimic_shock.csv', index=False)\n","    del ndf_ni, sdf_ni, ndf, sdf\n","\n","def carryFwd_minutes(opts, df, feat, cftime):\n","\n","    # Get the observed time\n","    df.loc[pd.notnull(df[feat]),'observedTime'] = df.loc[:,opts.timestamp_variable]\n","    \n","    # Forward filling with observed values\n","    df.loc[:,feat] = df.groupby(opts.unique_id_variable)[feat].ffill()\n","    # Forward filling with observed time\n","    df.loc[:,'observedTime'] = df.groupby(opts.unique_id_variable)['observedTime'].ffill()\n","    # Set NA for the imputed values out of the given time frame\n","    df.loc[df[opts.timestamp_variable]- df['observedTime'] > cftime, feat] = np.nan    \n","    # ---------------------------------------------------------   \n","    \n","    df = df.drop('observedTime', axis = 1)\n","    \n","    return df\n","    \n","# imputation with training data \n","def meanImpute(df, feat, meanTrain):\n","    for i in range(len(feat)):\n","        df[feat[i]] = df[feat[i]].fillna(meanTrain[i])\n","    return df\n","\n","    \n","# Main function\n","def main():\n","    start = time.time()\n","    \n","    #opts = Options()\n","    print(\"feat:\", opts.numerical_feat)\n","\n","    expertImputation(opts)   # Added : Missing data handling \n","    \n","    pos_events = pd.read_csv(opts.ts_pos_filepath)\n","    pos_events.loc[:,'EventTime'] = pos_events.ShockTime \n","    neg_events = pd.read_csv(opts.ts_neg_filepath)\n","    \n","    if True:\n","        neg_events.loc[:,'LastMinute'] = neg_events.groupby(opts.unique_id_variable).tail(1)[opts.timestamp_variable]\n","        neg_events['LastMinute'] = neg_events.groupby(opts.unique_id_variable)['LastMinute'].bfill()\n","        \n","        # MSS = Multivariate State Sequence\n","        MSS_pos, MSS_neg = make_MSS(pos_events, neg_events, opts)\n","    else:\n","        MSS_pos, MSS_neg = load_MSS(opts)\n","        \n","    accuracy, precision, recall, f_measure, auc = pred_fold(MSS_pos, MSS_neg, opts)\n","\n","    print(\"process time: {:.1f} min\".format((time.time()-start)/60))\n","    print(\"Results: \", accuracy, precision, recall, f_measure, auc)\n","    return [accuracy, precision, recall, f_measure, auc, opts.classifier, opts.kernl, opts.num_folds]\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u9-ynnN6YNnO","executionInfo":{"status":"ok","timestamp":1671070628551,"user_tz":480,"elapsed":1704,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"94314cc9-ebc9-4741-fc0d-19c8ad9b9116"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!ls /content/gdrive/MyDrive/mimic/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvbAEqRrit2w","executionInfo":{"status":"ok","timestamp":1671070648320,"user_tz":480,"elapsed":295,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"12ca9662-16d9-43f9-800c-113471816624"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["mimic_nonshock.csv\t     mimic_shock.csv\n","mimic_nonshock_noImpute.csv  mimic_shock_noImpute.csv\n"]}]},{"cell_type":"code","source":["\n","import tensorflow as tf\n","from IPython import get_ipython\n","from math import sqrt\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import mean_squared_error\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM\n","\n","import TemporalAbstraction\n","import RTPmining\n","import RTP\n","import classifier\n","from Config import Options\n","import time\n"],"metadata":{"id":"W_W_PJKadvIh","executionInfo":{"status":"ok","timestamp":1671070652240,"user_tz":480,"elapsed":367,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["sess = tf.compat.v1.InteractiveSession()\n","tf.random.set_seed(2022)\n","batch_size = [64, 128, 256]\n","opts = Options()"],"metadata":{"id":"_MIljiYid1tt","executionInfo":{"status":"ok","timestamp":1671070654845,"user_tz":480,"elapsed":327,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["## lets manually load the data and get a matrix of ones and zeros then well pad to specs\n","expertImputation(opts)   # Added : Missing data handling \n","    \n","pos_events = pd.read_csv(opts.ts_pos_filepath)\n","pos_events.loc[:,'EventTime'] = pos_events.ShockTime \n","\n","neg_events = pd.read_csv(opts.ts_neg_filepath)\n","neg_events.loc[:,'LastMinute'] = neg_events.groupby(opts.unique_id_variable).tail(1)[opts.timestamp_variable]\n","neg_events['LastMinute'] = neg_events.groupby(opts.unique_id_variable)['LastMinute'].bfill()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"luB5u1tUd7cZ","executionInfo":{"status":"error","timestamp":1671070789365,"user_tz":480,"elapsed":9077,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"3ddc9c3d-b7f6-4555-c78a-2fd8de3328ab"},"execution_count":69,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-b1811d93da8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexpertImputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Added : Missing data handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpos_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts_pos_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpos_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'EventTime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mShockTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mimic/mimic_shock.csv'"]}]},{"cell_type":"code","source":["##manually setting features we want to observe - before transformation\n","##we dont need all feilds\n","pos2_events = pos_events[['VisitIdentifier','MinutesFromArrival','SystolicBP','DiastolicBP','HeartRate','RespiratoryRate','Temperature','WBC','EventTime' ]]\n","neg2_events = neg_events[['VisitIdentifier','MinutesFromArrival','SystolicBP','DiastolicBP','HeartRate','RespiratoryRate','Temperature','WBC','LastMinute']]"],"metadata":{"id":"bNHr3ye-d9f6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##we got our data.. drop first two columns,and last \n","##concat the two as we have very biased data\n","dataset = pd.concat([pos2_events, neg2_events], ignore_index=True)\n","\n","dataset.columns.values\n"],"metadata":{"id":"ugk64iXjeCAZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = dataset[opts.numerical_feat][:]"],"metadata":{"id":"OCadAnuAeGym"},"execution_count":null,"outputs":[]}]}