{"cells":[{"cell_type":"markdown","metadata":{"id":"utqHsnqUGAx-"},"source":["<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n","\n","<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"]},{"cell_type":"markdown","metadata":{"id":"YPAKPdSuGAyC"},"source":["## Objective\n"]},{"cell_type":"markdown","metadata":{"id":"jWj_iMOuGAyD"},"source":["In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"]},{"cell_type":"markdown","metadata":{"id":"CyzdGm9EGAyE"},"source":["## Table of Contents\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","<font size = 3>\n","    \n","1. <a href=\"#item31\">Import Libraries and Packages</a>\n","2. <a href=\"#item32\">Download Data</a>  \n","3. <a href=\"#item33\">Define Global Constants</a>  \n","4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n","5. <a href=\"#item35\">Compile and Fit Model</a>\n","\n","</font>\n","    \n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"oT3iIhEUGAyF"},"source":["   \n"]},{"cell_type":"markdown","metadata":{"id":"juPNsZN7GAyG"},"source":["<a id='item31'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"mxzqpo2PGAyI"},"source":["## Import Libraries and Packages\n"]},{"cell_type":"markdown","metadata":{"id":"dGFETDDPGAyI"},"source":["Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"]},{"cell_type":"code","source":["!pip install skillsnetwork"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJs2sWoRGcMm","executionInfo":{"status":"ok","timestamp":1716680635759,"user_tz":420,"elapsed":18895,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"0961e227-09ab-4c23-cace-69462ee3885c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting skillsnetwork\n","  Downloading skillsnetwork-0.21.9-py3-none-any.whl (26 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from skillsnetwork) (7.34.0)\n","Collecting ipywidgets<9,>=8 (from skillsnetwork)\n","  Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from skillsnetwork) (2.31.0)\n","Requirement already satisfied: tqdm<5,>=4 in /usr/local/lib/python3.10/dist-packages (from skillsnetwork) (4.66.4)\n","Collecting comm>=0.1.3 (from ipywidgets<9,>=8->skillsnetwork)\n","  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=8->skillsnetwork) (5.7.1)\n","Collecting widgetsnbextension~=4.0.10 (from ipywidgets<9,>=8->skillsnetwork)\n","  Downloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jupyterlab-widgets~=3.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=8->skillsnetwork) (3.0.10)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (67.7.2)\n","Collecting jedi>=0.16 (from ipython->skillsnetwork)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->skillsnetwork) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->skillsnetwork) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->skillsnetwork) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->skillsnetwork) (2024.2.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->skillsnetwork) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->skillsnetwork) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->skillsnetwork) (0.2.13)\n","Installing collected packages: widgetsnbextension, jedi, comm, ipywidgets, skillsnetwork\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.6\n","    Uninstalling widgetsnbextension-3.6.6:\n","      Successfully uninstalled widgetsnbextension-3.6.6\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","Successfully installed comm-0.2.2 ipywidgets-8.1.2 jedi-0.19.1 skillsnetwork-0.21.9 widgetsnbextension-4.0.10\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"tags":[],"id":"PaPYfTqtGAyJ","executionInfo":{"status":"ok","timestamp":1716680635760,"user_tz":420,"elapsed":13,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["import skillsnetwork"]},{"cell_type":"markdown","metadata":{"id":"zCrJEdxJGAyM"},"source":["First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[],"id":"Ia4yXHW8GAyM","executionInfo":{"status":"ok","timestamp":1716680639490,"user_tz":420,"elapsed":3738,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{"id":"MDQjVRWuGAyO"},"source":["In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"tags":[],"id":"x3QVvjSlGAyP","executionInfo":{"status":"ok","timestamp":1716680639490,"user_tz":420,"elapsed":14,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense"]},{"cell_type":"markdown","metadata":{"id":"exchz2Q-GAyR"},"source":["Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[],"id":"Ox7wwYziGAyR","executionInfo":{"status":"ok","timestamp":1716680639491,"user_tz":420,"elapsed":14,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["from keras.applications import ResNet50\n","from keras.applications.resnet50 import preprocess_input"]},{"cell_type":"markdown","metadata":{"id":"wrKoNPgAGAyR"},"source":["<a id='item32'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"yeHgm7kIGAyS"},"source":["## Download Data\n"]},{"cell_type":"markdown","metadata":{"id":"J2ODyNr4GAyT"},"source":["In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":34,"referenced_widgets":["57559b27cbdb4d37b792b99677c70493","b8f08d67ea1f4e0ca732ce2e0e9c618d","8e4bca7d3fb741c2914e338ffe9ad569","4dc0d1a72700436f8782064033c76249","7a654f069cd34a67823078d5c2bb89d8","b245881fe4e04dc19e4eb2a1114a75cc","efd4d79910f14dcf9e93a2e8bb99bf0f","76e48bcd0af44b36991634efa27368b4","4134e0b11ee4465bbdee8fa3601b03d6","12414c38ba8748d284f38a2f1b175ba8","57c99a17ea31485ea89d120ec43aa80c","fb2619f7ab8a46089334a1fdd6e03041","c9b9b12dc8e74abba06949c1f9447915","eb4853cfb80141e89b67dcd2d3d5e533","141647718b0848c18c0ca9adb5b6a587","fadd78b76f7c48078d80e4c6b9139512","accc4c140d144ddea12c72ab201fc0d0","693bd472732e482384e1a42b1c4a1525","58fc7c4a506d458a82b51d4716ee8fe5","51586db8eb2440b08731e499265fa7c8","b801fc440af64760b7bdd67f2d72e752","34486090031f47649dc1248eccf401ea"]},"id":"JtmXcSQ3GAyT","executionInfo":{"status":"ok","timestamp":1716680656270,"user_tz":420,"elapsed":16791,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"005cb6a8-58ff-4267-95a1-051693493482"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57559b27cbdb4d37b792b99677c70493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/30036 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb2619f7ab8a46089334a1fdd6e03041"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved to '.'\n"]}],"source":["## get the data\n","await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"]},{"cell_type":"markdown","metadata":{"id":"aRcQ_FAuGAyV"},"source":["Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"]},{"cell_type":"markdown","metadata":{"id":"L6TB-DUBGAyV"},"source":["**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"]},{"cell_type":"markdown","metadata":{"id":"oU5ECHS8GAyW"},"source":["<a id='item33'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"8nY8AO-jGAyW"},"source":["## Define Global Constants\n"]},{"cell_type":"markdown","metadata":{"id":"_phkUzdGGAyW"},"source":["Here, we will define constants that we will be using throughout the rest of the lab.\n","\n","1. We are obviously dealing with two classes, so *num_classes* is 2.\n","2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n","3. We will training and validating the model using batches of 100 images.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"tags":[],"id":"U1hbkGK_GAyX","executionInfo":{"status":"ok","timestamp":1716680656270,"user_tz":420,"elapsed":10,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["num_classes = 2\n","\n","image_resize = 224\n","\n","batch_size_training = 100\n","batch_size_validation = 100"]},{"cell_type":"markdown","metadata":{"id":"4jN0StGNGAyY"},"source":["<a id='item34'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"3_eRvMzQGAyZ"},"source":["## Construct ImageDataGenerator Instances\n"]},{"cell_type":"markdown","metadata":{"id":"MMevoyweGAyZ"},"source":["In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"tags":[],"id":"mcDNn-6BGAya","executionInfo":{"status":"ok","timestamp":1716680656271,"user_tz":420,"elapsed":9,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n",")"]},{"cell_type":"markdown","metadata":{"id":"W-rVBdCkGAya"},"source":["Next, we will use the *flow_from_directory* method to get the training images as follows:\n"]},{"cell_type":"code","execution_count":9,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"04iKZhqNGAyb","executionInfo":{"status":"ok","timestamp":1716680656698,"user_tz":420,"elapsed":434,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"c96cb861-2347-4dac-f716-bc40fd1961c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 10001 images belonging to 2 classes.\n"]}],"source":["train_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/train',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')"]},{"cell_type":"markdown","metadata":{"id":"BxXEQb9fGAyb"},"source":["**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"]},{"cell_type":"markdown","metadata":{"id":"JcQhUfMuGAyc"},"source":["**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"cayaNRcyGAyd","executionInfo":{"status":"ok","timestamp":1716680656698,"user_tz":420,"elapsed":9,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"83f5c30c-be64-45a0-e5d6-9d5e33bf2b9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["train  valid\n"]}],"source":["!ls concrete_data_week3"]},{"cell_type":"code","execution_count":11,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"cgQn8EKtGAyd","executionInfo":{"status":"ok","timestamp":1716680656875,"user_tz":420,"elapsed":181,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"8295c0f8-1ca6-4a48-a189-984507ea5d98"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5001 images belonging to 2 classes.\n"]}],"source":["## Type your answer here\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')"]},{"cell_type":"markdown","metadata":{"id":"zANGvctcGAyf"},"source":["Double-click __here__ for the solution.\n","<!-- The correct answer is:\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')\n","-->\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TlgGpyjEGAyf"},"source":["<a id='item35'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"hnaah1lIGAyf"},"source":["## Build, Compile and Fit Model\n"]},{"cell_type":"markdown","metadata":{"id":"InsQ4ebwGAyg"},"source":["In this section, we will start building our model. We will use the Sequential model class from Keras.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"tags":[],"id":"Txt273q2GAyg","executionInfo":{"status":"ok","timestamp":1716680657846,"user_tz":420,"elapsed":975,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["model = Sequential()"]},{"cell_type":"markdown","metadata":{"id":"cSUNRDPhGAyh"},"source":["Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"mCFrHHXgGAyh","executionInfo":{"status":"ok","timestamp":1716680661169,"user_tz":420,"elapsed":3334,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"80343b58-c70a-4170-ca77-5d39cf4330e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 1s 0us/step\n"]}],"source":["model.add(ResNet50(\n","    include_top=False,\n","    pooling='avg',\n","    weights='imagenet',\n","    ))"]},{"cell_type":"markdown","metadata":{"id":"m2tHPPuMGAyh"},"source":["Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"tags":[],"id":"SHAMF_sCGAyh","executionInfo":{"status":"ok","timestamp":1716680661174,"user_tz":420,"elapsed":69,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"knykIpiiGAyi"},"source":["You can access the model's layers using the *layers* attribute of our model object.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"9L9dA5eSGAyi","executionInfo":{"status":"ok","timestamp":1716680661176,"user_tz":420,"elapsed":67,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"67df03d1-16aa-4ed6-fb7a-9205d1db3d30"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<keras.src.engine.functional.Functional at 0x7a298f40ee90>,\n"," <keras.src.layers.core.dense.Dense at 0x7a2990e6ed10>]"]},"metadata":{},"execution_count":15}],"source":["model.layers"]},{"cell_type":"markdown","metadata":{"id":"3FFTVMqbGAyi"},"source":["You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"]},{"cell_type":"markdown","metadata":{"id":"rUEL63N0GAyk"},"source":["You can access the ResNet50 layers by running the following:\n"]},{"cell_type":"code","execution_count":16,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"zascavr1GAyk","executionInfo":{"status":"ok","timestamp":1716680661177,"user_tz":420,"elapsed":62,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"f6d20a88-1d8a-4488-c1a6-e83281365a63"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<keras.src.engine.input_layer.InputLayer at 0x7a298ef84910>,\n"," <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7a298ef84700>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298ef85090>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298ef85de0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298ef86a40>,\n"," <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7a298ef85b40>,\n"," <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x7a298ef878b0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990ee1ba0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a2990ee1e10>,\n"," <keras.src.layers.core.activation.Activation at 0x7a2990ee3a00>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990ee3eb0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a2990ee2ad0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a2990ee3910>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990ee0670>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990ef5ed0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298ef87e80>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a2990ef4f70>,\n"," <keras.src.layers.merging.add.Add at 0x7a2990ef6d70>,\n"," <keras.src.layers.core.activation.Activation at 0x7a2990ef7df0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990f006d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a2990f01210>,\n"," <keras.src.layers.core.activation.Activation at 0x7a2990f00df0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990f022f0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a2990f02a10>,\n"," <keras.src.layers.core.activation.Activation at 0x7a2990f03820>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990f20610>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a2990f20ac0>,\n"," <keras.src.layers.merging.add.Add at 0x7a2990f03640>,\n"," <keras.src.layers.core.activation.Activation at 0x7a2990f22560>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990f23400>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a2990f23520>,\n"," <keras.src.layers.core.activation.Activation at 0x7a2990f23ca0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f094550>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f094a30>,\n"," <keras.src.layers.core.activation.Activation at 0x7a2990f21de0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990f20fd0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a2990f21c60>,\n"," <keras.src.layers.merging.add.Add at 0x7a2990ee36a0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298ef876a0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f097f10>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f095120>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f096c50>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0aa170>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0a97b0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f0abd60>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a2990f03e20>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0ab130>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f095750>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0ac220>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f0abc70>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f0ae5c0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0aeb30>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0af520>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f0ae590>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0c4a30>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0c4bb0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f0c55d0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0c4520>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0c6ec0>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f0c7eb0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f0e47c0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0e5900>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0e5690>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f0e6b90>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0e7040>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0e7fa0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f0e4bb0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0fcee0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0fd360>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f0fe4a0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f0ff850>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0fe110>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0c6800>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f0c48e0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0af580>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f095150>,\n"," <keras.src.layers.core.activation.Activation at 0x7a2990ee2230>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f11c760>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f11c8e0>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f0aa1a0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f11e6b0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f11f9d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f125900>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f124ee0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f1269b0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f1270d0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f1263e0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f11f4f0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f1278b0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f11fa30>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f1409d0>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f125ae0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f1431c0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f140cd0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f143fa0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f143e20>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f15d630>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f15d8d0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f15e140>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f15d120>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f15f6d0>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f15e5c0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f17d5a0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f15ee30>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f15ec20>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f15e950>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f11c130>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f0fe2c0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f11e530>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f0fe470>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f3903a0>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f143d90>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f390550>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f391db0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f3909a0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f3934f0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3939a0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f392a10>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f392c20>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3ad9c0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f3aded0>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f3af430>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f3adba0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3bc370>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f3bc2b0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f3ae110>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3be7a0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f3bdd50>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f3bf5e0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3bf580>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f3d4310>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f3bece0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f3d6ad0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3d71f0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f3d7970>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f3d6cb0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3f0ee0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f3d6530>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f3d4400>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3bd0f0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f390190>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f390a90>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f17fc70>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3f32e0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f3f3130>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f3f0880>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f401420>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f401360>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f3f03a0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f3bf340>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f400640>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f17d540>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f401630>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f3910f0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f403970>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f40de70>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f40e500>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f40d300>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f40d360>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f40fa00>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f40f910>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f40e260>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f40ea70>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f40e7a0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f429e70>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f429330>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f42b490>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f42af20>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f42bdf0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f429060>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f42b9a0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x7a298f448670>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x7a298f44a800>,\n"," <keras.src.layers.merging.add.Add at 0x7a298f42a4d0>,\n"," <keras.src.layers.core.activation.Activation at 0x7a298f429ae0>,\n"," <keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x7a298f42a050>]"]},"metadata":{},"execution_count":16}],"source":["model.layers[0].layers"]},{"cell_type":"markdown","metadata":{"id":"u6G1owenGAyk"},"source":["Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"tags":[],"id":"-wuyShOcGAyl","executionInfo":{"status":"ok","timestamp":1716680661178,"user_tz":420,"elapsed":56,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["model.layers[0].trainable = False"]},{"cell_type":"markdown","metadata":{"id":"Bal9ohsvGAyl"},"source":["And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"]},{"cell_type":"code","execution_count":18,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"nDeuGoB4GAym","executionInfo":{"status":"ok","timestamp":1716680661179,"user_tz":420,"elapsed":54,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"c53e7fc5-ba80-4b44-f4e7-5ad4e108d77b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 2048)              23587712  \n","                                                                 \n"," dense (Dense)               (None, 2)                 4098      \n","                                                                 \n","=================================================================\n","Total params: 23591810 (90.00 MB)\n","Trainable params: 4098 (16.01 KB)\n","Non-trainable params: 23587712 (89.98 MB)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"NUHVaBVcGAym"},"source":["Next we compile our model using the **adam** optimizer.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"tags":[],"id":"IM_kw5FKGAyn","executionInfo":{"status":"ok","timestamp":1716680661180,"user_tz":420,"elapsed":38,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"OrbsmZnjGAyo"},"source":["Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"]},{"cell_type":"code","execution_count":20,"metadata":{"tags":[],"id":"loxC2d0gGAyo","executionInfo":{"status":"ok","timestamp":1716680661180,"user_tz":420,"elapsed":35,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}}},"outputs":[],"source":["steps_per_epoch_training = len(train_generator)\n","steps_per_epoch_validation = len(validation_generator)\n","num_epochs = 2"]},{"cell_type":"markdown","metadata":{"id":"KDNtyCjWGAyo"},"source":["Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"]},{"cell_type":"code","execution_count":21,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnFvBJfzGAyo","executionInfo":{"status":"ok","timestamp":1716680796594,"user_tz":420,"elapsed":135447,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"7f085176-91bc-4b02-d0be-6f628ccfeda4"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-21-172e67583a70>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  fit_history = model.fit_generator(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","101/101 [==============================] - 61s 501ms/step - loss: 0.1209 - accuracy: 0.9562 - val_loss: 0.0237 - val_accuracy: 0.9942\n","Epoch 2/2\n","101/101 [==============================] - 49s 485ms/step - loss: 0.0139 - accuracy: 0.9971 - val_loss: 0.0148 - val_accuracy: 0.9954\n"]}],"source":["fit_history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=steps_per_epoch_training,\n","    epochs=num_epochs,\n","    validation_data=validation_generator,\n","    validation_steps=steps_per_epoch_validation,\n","    verbose=1,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Xtw_hsWmGAyp"},"source":["Now that the model is trained, you are ready to start using it to classify images.\n"]},{"cell_type":"markdown","metadata":{"id":"Bbr0IS7TGAyp"},"source":["Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"Smc96TDkGAyp","executionInfo":{"status":"ok","timestamp":1716680796895,"user_tz":420,"elapsed":314,"user":{"displayName":"Shames Y","userId":"05526037345958655415"}},"outputId":"03963960-7491-407c-aa9b-584abd399f36"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["model.save('classifier_resnet_model.h5')"]},{"cell_type":"markdown","metadata":{"id":"X8BlfORWGAyq"},"source":["Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"]},{"cell_type":"markdown","metadata":{"id":"elY-PpVNGAyq"},"source":["### Thank you for completing this lab!\n","\n","This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"]},{"cell_type":"markdown","metadata":{"id":"WcRdH7qfGAyq"},"source":["This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"]},{"cell_type":"markdown","metadata":{"id":"C9j45XHBGAyq"},"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VV1ESPdsGAyr"},"source":["<hr>\n","\n","Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"57559b27cbdb4d37b792b99677c70493":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8f08d67ea1f4e0ca732ce2e0e9c618d","IPY_MODEL_8e4bca7d3fb741c2914e338ffe9ad569","IPY_MODEL_4dc0d1a72700436f8782064033c76249"],"layout":"IPY_MODEL_7a654f069cd34a67823078d5c2bb89d8","tabbable":null,"tooltip":null}},"b8f08d67ea1f4e0ca732ce2e0e9c618d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b245881fe4e04dc19e4eb2a1114a75cc","placeholder":"​","style":"IPY_MODEL_efd4d79910f14dcf9e93a2e8bb99bf0f","tabbable":null,"tooltip":null,"value":"Downloading concrete_data_week3.zip: 100%"}},"8e4bca7d3fb741c2914e338ffe9ad569":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_76e48bcd0af44b36991634efa27368b4","max":97863179,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4134e0b11ee4465bbdee8fa3601b03d6","tabbable":null,"tooltip":null,"value":97863179}},"4dc0d1a72700436f8782064033c76249":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_12414c38ba8748d284f38a2f1b175ba8","placeholder":"​","style":"IPY_MODEL_57c99a17ea31485ea89d120ec43aa80c","tabbable":null,"tooltip":null,"value":" 97863179/97863179 [00:03&lt;00:00, 31488056.47it/s]"}},"7a654f069cd34a67823078d5c2bb89d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b245881fe4e04dc19e4eb2a1114a75cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efd4d79910f14dcf9e93a2e8bb99bf0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"76e48bcd0af44b36991634efa27368b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4134e0b11ee4465bbdee8fa3601b03d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12414c38ba8748d284f38a2f1b175ba8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57c99a17ea31485ea89d120ec43aa80c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"fb2619f7ab8a46089334a1fdd6e03041":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9b9b12dc8e74abba06949c1f9447915","IPY_MODEL_eb4853cfb80141e89b67dcd2d3d5e533","IPY_MODEL_141647718b0848c18c0ca9adb5b6a587"],"layout":"IPY_MODEL_fadd78b76f7c48078d80e4c6b9139512","tabbable":null,"tooltip":null}},"c9b9b12dc8e74abba06949c1f9447915":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_accc4c140d144ddea12c72ab201fc0d0","placeholder":"​","style":"IPY_MODEL_693bd472732e482384e1a42b1c4a1525","tabbable":null,"tooltip":null,"value":"Extracting concrete_data_week3.zip: 100%"}},"eb4853cfb80141e89b67dcd2d3d5e533":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_58fc7c4a506d458a82b51d4716ee8fe5","max":30036,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51586db8eb2440b08731e499265fa7c8","tabbable":null,"tooltip":null,"value":30036}},"141647718b0848c18c0ca9adb5b6a587":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b801fc440af64760b7bdd67f2d72e752","placeholder":"​","style":"IPY_MODEL_34486090031f47649dc1248eccf401ea","tabbable":null,"tooltip":null,"value":" 30036/30036 [00:11&lt;00:00, 3084.06it/s]"}},"fadd78b76f7c48078d80e4c6b9139512":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"accc4c140d144ddea12c72ab201fc0d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"693bd472732e482384e1a42b1c4a1525":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"58fc7c4a506d458a82b51d4716ee8fe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51586db8eb2440b08731e499265fa7c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b801fc440af64760b7bdd67f2d72e752":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34486090031f47649dc1248eccf401ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}}}},"nbformat":4,"nbformat_minor":0}